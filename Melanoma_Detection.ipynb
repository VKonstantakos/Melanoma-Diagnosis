{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Melanoma-Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IKdiZf9oCCX6",
        "jVDy9wwS3-s7",
        "kAI126NuA8t6",
        "KKM4nHbrplUy",
        "z3kE-JST-9Xu",
        "MTI5P7VR5mea",
        "7tUEgv7zzdgv",
        "6D1Neln8zdgz",
        "MLggd3A4Fm3s",
        "UiHqaDtWF04v",
        "lQ0iHis9qUxV"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKdiZf9oCCX6"
      },
      "source": [
        "# Prepare Project\n",
        "\n",
        "Can be run on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVDy9wwS3-s7"
      },
      "source": [
        "## Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCFBFzbnYXHE"
      },
      "source": [
        "# Libraries to download datasets\n",
        "!pip install sla-cli\n",
        "!pip install alive-progress\n",
        "!pip install patool\n",
        "!pip install fuzzywuzzy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi1P3hRNYX79"
      },
      "source": [
        "# Summary of datasets\n",
        "!sla-cli ls -v all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAI126NuA8t6"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfHWjuXE1Ozg"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viMbqptftSev"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Conv2D, GlobalAveragePooling2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization\n",
        "\n",
        "from tensorflow.keras.applications.nasnet import NASNetLarge\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB7\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK-egEK35jf8"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKM4nHbrplUy"
      },
      "source": [
        "### Visualization functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsxlyrxqoXLm"
      },
      "source": [
        "def show_diagnosis(dataset, id='image', random_state=None,\n",
        "                   training_path='/content/data/ISIC2018_Task3_Training_Input/'):\n",
        "  '''\n",
        "  Plots examples of each diagnosis in the training dataset.\n",
        "  '''\n",
        "\n",
        "  # Set random seed for reproducible results\n",
        "  random.seed(random_state)\n",
        "\n",
        "  # Create figure\n",
        "  fig = plt.figure(figsize=(12, 7))\n",
        "  plt.title('Diagnostic Examples', fontsize=13)\n",
        "  plt.grid()\n",
        "  plt.axis(False)\n",
        "  columns = 4\n",
        "  rows = 2\n",
        "  ax = []\n",
        "\n",
        "  # Diagnosis\n",
        "  diagnosis = dataset.drop(id, axis=1).columns\n",
        "\n",
        "  # Iterate for every diagnosis\n",
        "  for index, col in enumerate(diagnosis):\n",
        "    dataset_diagnosis = dataset.loc[dataset[col] == 1, id].values\n",
        "    idx = random.choice(dataset_diagnosis)\n",
        "    label = col\n",
        "\n",
        "    # Read image\n",
        "    img = plt.imread(training_path + idx + '.jpg')\n",
        "\n",
        "    # Create subplot and append to ax\n",
        "    ax.append(fig.add_subplot(rows, columns, index+1))\n",
        "\n",
        "    # Hide grid lines\n",
        "    ax[-1].grid(False)\n",
        "\n",
        "    # Hide axes ticks\n",
        "    ax[-1].set_xticks([])\n",
        "    ax[-1].set_yticks([])\n",
        "    ax[-1].set_title(label)\n",
        "    plt.imshow(img)\n",
        "\n",
        "  # Add one more example\n",
        "  col = random.choice(diagnosis)\n",
        "  dataset_diagnosis = dataset.loc[dataset[col] == 1, id].values\n",
        "  idx = random.choice(dataset_diagnosis)\n",
        "  label = col\n",
        "\n",
        "  # Read image\n",
        "  img = plt.imread(training_path + idx + '.jpg')\n",
        "\n",
        "  # Create subplot and append to ax\n",
        "  ax.append(fig.add_subplot(rows, columns, 8))\n",
        "\n",
        "  # Hide grid lines\n",
        "  ax[-1].grid(False)\n",
        "\n",
        "  # Hide axes ticks\n",
        "  ax[-1].set_xticks([])\n",
        "  ax[-1].set_yticks([])\n",
        "  ax[-1].set_title(label)\n",
        "  plt.imshow(img)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def plot_model_history(model_history, save=True):\n",
        "  '''\n",
        "  Plot model's validation loss and validation accuracy.\n",
        "  '''\n",
        "\n",
        "  fig, axs = plt.subplots(1,2,figsize=(16, 7))\n",
        "\n",
        "  # summarize history for accuracy\n",
        "  axs[0].plot(range(1, len(model_history.history['accuracy'])+1), model_history.history['accuracy'])\n",
        "  axs[0].plot(range(1, len(model_history.history['val_accuracy'])+1), model_history.history['val_accuracy'])\n",
        "  axs[0].set_title('Model Accuracy')\n",
        "  axs[0].set_ylabel('Accuracy')\n",
        "  axs[0].set_xlabel('Epoch')\n",
        "  axs[0].set_xticks(np.arange(1,len(model_history.history['accuracy'])+1),len(model_history.history['accuracy'])/10)\n",
        "  axs[0].legend(['Training', 'Validation'], loc='best')\n",
        "\n",
        "  # summarize history for loss\n",
        "  axs[1].plot(range(1, len(model_history.history['loss'])+1), model_history.history['loss'])\n",
        "  axs[1].plot(range(1, len(model_history.history['val_loss'])+1), model_history.history['val_loss'])\n",
        "  axs[1].set_title('Model Loss')\n",
        "  axs[1].set_ylabel('Loss')\n",
        "  axs[1].set_xlabel('Epoch')\n",
        "  axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "  axs[1].legend(['Training', 'Validation'], loc='best')\n",
        "\n",
        "  if save == True:\n",
        "      plt.savefig('Training history.png')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBDU0nS438Ae"
      },
      "source": [
        "# Main Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3kE-JST-9Xu"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnTvGz30g3pU"
      },
      "source": [
        "ISIC 2018 Dataset (HAM10000)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVgXOpmcjqfk"
      },
      "source": [
        "# Prepare directory to download data\n",
        "phases = ['Training', 'Validation', 'Test']\n",
        "data_root = os.path.join(os.getcwd(), 'data')\n",
        "data_dir = {phase:os.path.join(data_root, phase) for phase in phases}\n",
        "\n",
        "# Define download function\n",
        "def download(url, destination_folder='.'):\n",
        "  !wget -nc -q --show-progress $url -P $destination_folder\n",
        "\n",
        "# Download Training.zip, Validation.zip, Test.zip and ground truth labels\n",
        "for phase in phases:\n",
        "  download(f'https://isic-challenge-data.s3.amazonaws.com/2018/ISIC2018_Task3_{phase}_Input.zip', data_root)\n",
        "\n",
        "for phase in phases[:2]:\n",
        "  download(f'https://isic-challenge-data.s3.amazonaws.com/2018/ISIC2018_Task3_{phase}_GroundTruth.zip', data_root)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPEgl6JFALWA"
      },
      "source": [
        "# Unzip Training.zip, Validation.zip, Test.zip and ground truth labels\n",
        "for phase in phases:\n",
        "  if not os.path.exists(data_dir[phase]):\n",
        "    with zipfile.ZipFile(os.path.join(data_root, f'ISIC2018_Task3_{phase}_Input.zip'), 'r') as myzip:\n",
        "      for file in tqdm(myzip.namelist(), desc=f'Extracting {phase}.zip'):\n",
        "        myzip.extract(member=file, path=data_root)\n",
        "\n",
        "for phase in phases[:2]:\n",
        "  if not os.path.exists(data_dir[phase]):\n",
        "    with zipfile.ZipFile(os.path.join(data_root, f'ISIC2018_Task3_{phase}_GroundTruth.zip'), 'r') as myzip:\n",
        "      for file in tqdm(myzip.namelist(), desc=f'Extracting {phase}.zip'):\n",
        "        myzip.extract(member=file, path=data_root)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "025K4GpE517U"
      },
      "source": [
        "Download additional data: BCN20000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bodR6Es9_bU1"
      },
      "source": [
        "!sla-cli download bcn_20000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-AqAU6wzmD2"
      },
      "source": [
        "df = pd.read_csv('/content/bcn_20000/metadata.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brmI_yxW0YDB"
      },
      "source": [
        "diagn_map = {'melanoma': 'MEL', 'basal cell carcinoma': 'BCC', 'seborrheic keratosis': 'BKL',\n",
        "             'actinic keratosis': 'AKIEC', 'solar lentigo': 'BKL', 'dermatofibroma': 'DF',\n",
        "             'vascular lesion': 'VASC'}\n",
        "\n",
        "df['dx'] = df['dx'].map(diagn_map)\n",
        "df = df.dropna(subset=['dx'], axis=0).reset_index(drop=True)\n",
        "bcn_data = df[['image_name', 'dx']].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkcV8Ov25QnC"
      },
      "source": [
        "bcn_dir = '/content/bcn_20000/images'\n",
        "train_dir = '/content/data/ISIC2018_Task3_Training_Input'\n",
        "\n",
        "file_names = os.listdir(bcn_dir)\n",
        "\n",
        "for file_name in file_names:\n",
        "    shutil.move(os.path.join(bcn_dir, file_name), train_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAqZoLQpBHeF"
      },
      "source": [
        "PAD-UFES-20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnwOuImN6T3L"
      },
      "source": [
        "# Error in cleaning up doesn't affect the images\n",
        "!sla-cli download pad_ufes_20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpHvmpuXEBn5"
      },
      "source": [
        "df2 = pd.read_csv('/content/PAD_UFES_20/metadata.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNN36suGB6ip"
      },
      "source": [
        "diagn_map2 = {'BCC': 'BCC', 'ACK': 'AKIEC', 'SEK': 'BKL', 'MEL': 'MEL'}\n",
        "\n",
        "df2['diagnostic'] = df2['diagnostic'].map(diagn_map2)\n",
        "df2 = df2.dropna(subset=['diagnostic'], axis=0).reset_index(drop=True)\n",
        "pad_data = df2[['img_id', 'diagnostic']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWDifwsZDub4"
      },
      "source": [
        "pad_dir = '/content/PAD_UFES_20/images'\n",
        "train_dir = '/content/data/ISIC2018_Task3_Training_Input'\n",
        "    \n",
        "file_names = os.listdir(pad_dir)\n",
        "\n",
        "for file_name in file_names:\n",
        "    shutil.move(os.path.join(pad_dir, file_name), train_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTI5P7VR5mea"
      },
      "source": [
        "## Load and explore data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elZn-8YnwSG_"
      },
      "source": [
        "# Reading the datasets\n",
        "train_labels = pd.read_csv(os.path.join(data_root, 'ISIC2018_Task3_Training_GroundTruth/ISIC2018_Task3_Training_GroundTruth.csv'))\n",
        "val_labels = pd.read_csv(os.path.join(data_root, 'ISIC2018_Task3_Validation_GroundTruth/ISIC2018_Task3_Validation_GroundTruth.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff7tseHOki-J"
      },
      "source": [
        "# Integrate all data\n",
        "training_labels = train_labels.copy()\n",
        "training_labels['diagnosis'] = training_labels.iloc[:, 1:].idxmax(axis=1)\n",
        "training_labels['img_id'] = training_labels['image'] + '.jpg'\n",
        "train_data = training_labels[['img_id', 'diagnosis']]\n",
        "\n",
        "validation_labels = val_labels.copy()\n",
        "validation_labels['diagnosis'] = validation_labels.iloc[:, 1:].idxmax(axis=1)\n",
        "validation_labels['img_id'] = validation_labels['image'] + '.jpg'\n",
        "val_data = validation_labels[['img_id', 'diagnosis']]\n",
        "\n",
        "bcn_data['img_id'] =  bcn_data['image_name'] + '.jpg'\n",
        "bcn_data = bcn_data[['img_id', 'dx']]\n",
        "bcn_data = bcn_data.rename(columns={'dx':'diagnosis'})\n",
        "\n",
        "pad_data = pad_data.rename(columns={'diagnostic':'diagnosis'})\n",
        "\n",
        "# Create merged dataset\n",
        "train_all = pd.concat([train_data, bcn_data, pad_data], axis=0).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy4uN0vg7cmZ"
      },
      "source": [
        "Check for null values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wR7xY5F7SoJ"
      },
      "source": [
        "train_labels.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTpuGm8L7SrF"
      },
      "source": [
        "train_labels.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tUEgv7zzdgv"
      },
      "source": [
        "## Data Visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0T2Xh_WJKIs"
      },
      "source": [
        "Show class distribution before and after integrating additional data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6QV_aW4XCnv"
      },
      "source": [
        "# Count the number of diagnoses\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "plt.title('Distribution of skin diseases')\n",
        "sns.countplot(data=train_data, x='diagnosis')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUw65IiQIQ2b"
      },
      "source": [
        "# Count the number of diagnoses\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "plt.title('Distribution of skin diseases')\n",
        "sns.countplot(data=train_all, x='diagnosis')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi45HBAlJRMl"
      },
      "source": [
        "Compare percentages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR9ydTN-7kSq"
      },
      "source": [
        "# Show percentages\n",
        "print(f\"Initial percentage of each diagnosis:\\n{100*train_data['diagnosis'].value_counts(normalize=True).to_frame()}\")\n",
        "\n",
        "# Show percentages\n",
        "print(f\"\\nFinal percentage of each diagnosis:\\n{100*train_all['diagnosis'].value_counts(normalize=True).to_frame()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D1Neln8zdgz"
      },
      "source": [
        "## Diagnosis Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug99kmQonpMu"
      },
      "source": [
        "show_diagnosis(train_labels, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLggd3A4Fm3s"
      },
      "source": [
        "## Image Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL0SohPCDS6I"
      },
      "source": [
        "# Set 0 if you want to use EfficientNet or other models with input 224.\n",
        "# Set 1 if you want to use InceptionV3 or other models with input 299.\n",
        "model_id = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pagt94hsl0di"
      },
      "source": [
        "# Load data in batches with an ImageDataGenerator\n",
        "\n",
        "datagen_train = ImageDataGenerator(\n",
        "                    rotation_range=20,\n",
        "                    width_shift_range=0.1,\n",
        "                    height_shift_range=0.1,\n",
        "                    brightness_range=None, \n",
        "                    shear_range=0.2,\n",
        "                    zoom_range=0.2,\n",
        "                    channel_shift_range=0.0, \n",
        "                    fill_mode='nearest', \n",
        "                    cval=0.0, \n",
        "                    horizontal_flip=True,\n",
        "                    vertical_flip=True,\n",
        "                    rescale=1.0/255.0, \n",
        "                    preprocessing_function=None,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "datagen_test = ImageDataGenerator(\n",
        "                    rescale=1.0/255.0)\n",
        "\n",
        "# Shape of the images (lxl)\n",
        "if model_id is 0:\n",
        "  l = 224\n",
        "else:\n",
        "  l = 299\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# Training Generators\n",
        "train_batches = datagen_train.flow_from_dataframe(dataframe=train_all, directory=os.path.join(data_root, 'ISIC2018_Task3_Training_Input'),\n",
        "                                                  x_col='img_id', y_col='diagnosis', class_mode='categorical', batch_size=batch_size, shuffle=True,\n",
        "                                                  target_size= (l, l), subset='training')\n",
        "\n",
        "valid_batches = datagen_train.flow_from_dataframe(dataframe=train_all, directory=os.path.join(data_root, 'ISIC2018_Task3_Training_Input'),\n",
        "                                                  x_col='img_id', y_col='diagnosis', class_mode='categorical', batch_size=batch_size, shuffle=True,\n",
        "                                                  target_size= (l, l), subset='validation')\n",
        "\n",
        "# Evaluation Generators\n",
        "test_batches = datagen_test.flow_from_directory(data_root, batch_size=batch_size,\n",
        "                                                target_size=(l, l), shuffle=False, classes = ['ISIC2018_Task3_Test_Input'])\n",
        "\n",
        "valid_ind = datagen_test.flow_from_dataframe(dataframe=val_data, directory=os.path.join(data_root, 'ISIC2018_Task3_Validation_Input'),\n",
        "                                             x_col='img_id', y_col='diagnosis', class_mode='categorical', batch_size=batch_size,\n",
        "                                             shuffle=False, target_size= (l, l))\n",
        "\n",
        "\n",
        "batchX, batchy = train_batches.next()\n",
        "print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiHqaDtWF04v"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "Create model from scratch to serve as a baseline classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s40WBhx24TK"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\", input_shape=(224, 224, 3)))\n",
        "model.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\"))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
        "model.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256,activation=\"relu\"))\n",
        "\n",
        "model.add(Dense(7 ,activation=\"softmax\"))\n",
        "\n",
        "optimizer=optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=['accuracy', 'AUC'], weighted_metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Iuh-w87P-UJ"
      },
      "source": [
        "# Callback functions\n",
        "\n",
        "# Earlystop\n",
        "earlystop_callback = EarlyStopping(monitor = 'val_loss',\n",
        "                                   min_delta = 0,\n",
        "                                   patience = 20,\n",
        "                                   verbose = 1,\n",
        "                                   restore_best_weights = True)\n",
        "\n",
        "# Save the most accurate model's weights\n",
        "checkpoint_callback = ModelCheckpoint(filepath='/content/',\n",
        "                                      save_weights_only=True,\n",
        "                                      monitor='val_weighted_accuracy',\n",
        "                                      mode='max',\n",
        "                                      save_best_only=True)\n",
        "\n",
        "my_callbacks = [earlystop_callback, checkpoint_callback]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4i4VXYxm1XB"
      },
      "source": [
        "# Weighted loss\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(train_all['diagnosis']),\n",
        "                                                 train_all['diagnosis'])\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "print(np.unique(train_all['diagnosis']))\n",
        "print(class_weights)\n",
        "\n",
        "custom_weights = {0: 1.5, 1: 1.0, 2: 1.0, 3: 10.0, 4: 1.1, 5: 0.4, 6: 10.0}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iTLcohipMGi"
      },
      "source": [
        "# Train the model\n",
        "history = model.fit(train_batches, validation_data=valid_batches, epochs = 100, callbacks = my_callbacks,\n",
        "                    shuffle = True, steps_per_epoch = 50, verbose=2, class_weight=class_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qULFyy4Kpyd7"
      },
      "source": [
        "# Save best model's weights\n",
        "model.save_weights('SimpleCNN.hdf5')\n",
        "\n",
        "# Plot training history\n",
        "plot_model_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ0iHis9qUxV"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HvLdVhJuveE"
      },
      "source": [
        "# Load model\n",
        "model.load_weights('/content/SimpleCNN.hdf5')\n",
        "\n",
        "# Evaluate the model on the validation dataset\n",
        "\n",
        "results = model.evaluate(valid_ind, batch_size=batch_size)\n",
        "print(\"Test loss: {:.3f}\".format(results[0]))\n",
        "print(\"Test accuracy: {:.3f}\".format(results[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inLNhjmlq7Iz"
      },
      "source": [
        "# Get predictions on the test dataset\n",
        "results = model.predict(test_batches, batch_size=batch_size)\n",
        "\n",
        "# Transform predictions on the appropriate format for submission\n",
        "df_pred = pd.DataFrame(results)\n",
        "df_pred = df_pred.rename(columns={0:'AKIEC', 1:'BCC', 2:'BKL', 3:'DF', 4:'MEL', 5:'NV', 6:'VASC'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p_pMNPf2tHv"
      },
      "source": [
        "ids = []\n",
        "for filename in os.listdir('/content/data/Test/ISIC2018_Task3_Test_Input'):\n",
        "  if os.path.splitext(filename)[1] == '.jpg':\n",
        "    id = os.path.splitext(filename)[0]\n",
        "    ids.append(id)\n",
        "\n",
        "df_pred['image'] = ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp100gpR1AAM"
      },
      "source": [
        "df_pred.to_csv('InitialResults.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkIVDtk3599B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}